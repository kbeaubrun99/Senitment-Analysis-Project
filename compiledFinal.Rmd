---
title: "Sentiment Analysis with Airlines"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Import Libraries
```{r}
library(dplyr)
library(tidyr)
library(ggplot2)
library(data.table)
library(here)

library(igraph)
library(ggraph)
library(glue)
library(tidytext)
library(tidygraph)
library(tidyverse)
library(tm)
library(shiny)
library(wordcloud)

library(stringr)
library(textdata)
library(textstem)
library(colorspace)

options(scipen = 999) 

```

## Remove Scietific Notation & Set character encoding
```{r}
options(scipen = 999)
```

## Import Data
```{r}
raw_tweets <- read.csv(here::here("Tweets-1.csv"), header = TRUE, stringsAsFactors = FALSE) %>% janitor::clean_names()

fread(here::here("final project", "Tweets-1.csv"))

str(raw_tweets)

raw_tweets$text[1:10]

```
## Cleaning to-do 
1.tweet_id - Check tweet_id for duplicates - ensure each value is unique

2. airline_sentiment - Check all possible values of airline_senitment to ensure consistency

3. negative_reason - What kind of formatting are we looking for in negative reason. - Will this be used in analysis/What formatting is needed?

5. General question - How are we going to deal with missing values? What percentage of values are missing for each column? 

6. Text - need to remove "@VirginAmerica", special characters need to be removed/edited. 

7. Text - Stemming? Lemmatization? 

8. tweet_coord - Will this be used? If so what format? Break into two columns? 

9. tweet_created - Change to date type 


## Checking tweet id 
```{r}
raw_tweets %>%
  group_by(tweet_id) %>%
  tally()

```
Only 3 different values for tweet_id. Need to create unique_id for each tweet. 

## Check airline sentiment
```{r}

raw_tweets %>%
  group_by(airline_sentiment) %>%
  tally()

```

## Cleaning text 
```{r}


```
Change to UTF-8 - Usually standard for internet text 

```{r,"WordClouds and nGrams: Import function"}
importData <- function() {
  cleanTweetOrig <- read.csv(here::here("Clean Tweets April 6.csv"), header = TRUE, stringsAsFactors = FALSE) %>% janitor::clean_names()
  
  cleanTweetsDF <- cleanTweetOrig
  return(cleanTweetsDF)
}


```


```{r,"WordClouds and nGrams: Munging function"}

mungeData <- function (cleanTweetsDF) {
  # Extra precleaning specific to wordcloud/ngram output
  # b/c we don't want unnest tokens
  # to pick up links and various other things
  
  # remove links
  cleanTweetsDF$text <- gsub("http.*","",  cleanTweetsDF$text)
  cleanTweetsDF$text <- gsub("https.*","",  cleanTweetsDF$text)
  # remove punctuation
  cleanTweetsDF$text <- gsub("[[:punct:]]", "", cleanTweetsDF$text)
  # convert to lowercase
  cleanTweetsDF$text <- gsub(pattern = '([[:upper:]])', perl = TRUE, replacement = '\\L\\1', cleanTweetsDF$text)
  # remove stopwords
 # customStopWords <- c(stopwords(), "flight")
  customStopWords <- stopwords()
  cleanTweetsDF$text <- gsub(paste0('\\b',customStopWords, '\\b', collapse = '|'), '', cleanTweetsDF$text)
  
  # "stemming" truncate anything that starts with "fly" (i.e. "flying")
  # and anything that starts with "flight"
  cleanTweetsDF$text <- gsub("flight\\S*", "flight", cleanTweetsDF$text)
  cleanTweetsDF$text <- gsub("fly\\S*", "fly", cleanTweetsDF$text)
  
  return(cleanTweetsDF)
}


```

```{r, "WordClouds and nGrams: WordCount function"}
### WordCount
wordCount <- function(tweetsDF) {
  return(
    tweetsDF %>%
    unnest_tokens(input = text, output = word) %>%
    drop_na() %>%
    count(word, sort = TRUE)
    )
}
```

```{r, "Extract Hashclouds function"}
### hashtags

getHashTags <- function(tweetsDF) { 
  extractTags <- function(tweets) {
    hashVector <- str_extract_all(string = tweets, pattern = '#\\S+', simplify = TRUE) %>% as.character()
    if (length(hashVector) > 0 ) {
      return(hashVector %>% str_c(collapse=", ")) 
    } else {
      return(NA)
    }
  }
    hashTags <- tibble(text = cleanTweetsDF$text %>% map_chr(.f = ~ extractTags(tweets = .x)))
  return(hashTags)    
}


```




```{r, "WordClouds and nGrams plotting functions"}
### plots
#### wordcount


wordCountPlot <- function(theWords, number = 25) {
  theWords %>%
    #filter(n > threshold) %>% # from an earlier version, where I plotted
    # based on frequency
    mutate(word = reorder(word, n)) %>%
    head(number) %>%
    ggplot(aes(x = word, y = n)) +
    geom_col() +
    coord_flip() +
    ggtitle(label = glue('{number} most frequent words'))
} 

wordCloudPlot <- function(wordCounts, threshold = 10, scale = c(.5,2)) {
  wordcloud(
    words = wordCounts$word,
  freq = wordCounts$n,
  scale = c(.5,3),
  min.freq = threshold,
  colors = brewer.pal(5, 'Paired')
  )
}
  

```


```{r, "WordCloud and nGrams initialization"}
# initialize
cleanTweetsDF <- importData()
# REMINDER: munging removes punctuation. Get the hashtags out first!
hashTagsDF <- getHashTags()
cleanTweetsDF <- mungeData(cleanTweetsDF)

# get word counts
wordCounts <- wordCount(cleanTweetsDF)
hashCounts <- wordCount(hashTagsDF)

```
```{r, "Generate WordClouds", fig.width=6, fig.height=4}
#use out.width/height "###px" to control knit output 
# Word Clouds
# we kept the word flight as important to the nGrams, but it completely
# skews anything that might be interesting in a word cloud, so we remove
# it here
wordCloudPlot(wordCounts[-1,], threshold = 15, scale = c(0.25,1))
wordCloudPlot(wordCounts[-1,], threshold = 25, scale = c(0.5,1))
wordCloudPlot(hashCounts, threshold = 10, scale = c(0.25,.75))


```

```
#Clean data provided by Ean
raw_data <- fread(here::here("final project", "Tweets-1.csv"), integer64 = "character")

tidy_tweets <- 
  raw_tweets %>%
  select(-airline_sentiment_confidence, -airline_sentiment_gold, -negativereason_confidence, -negativereason_gold) %>%
  mutate(tweet_id = row_number(), 
         tweet_created = as.Date(tweet_created, "%m/%d/%Y %H:%M"),
         text = iconv(text, from = "UTF-8", to = "ASCII//TRANSLIT"),
         text = gsub(text, pattern = "[@]\\S\\w*", replacement = "")) %>%
  separate(tweet_coord, into = c("latitude", "longitude"), sep = ",") %>%
  mutate(latitude = gsub(latitude, pattern = "\\[", replacement = ""),
         longitude = gsub(longitude, pattern = "\\]", replacement = "")) %>%
  mutate(across(.cols = c("latitude", "longitude"), .fns = as.numeric))

str(tidy_tweets)
```

```{r}
# Create custom stopwords
custom_stopwords <- 
  tibble::tribble(~word,~lexicon,
                  "t.co", "custom",
                  "http", "custom")

# Get all stop words 
stop_words <- 
  stop_words %>%
  bind_rows(custom_stopwords)
```

```{r}
# Unnest tokens 
# Remove stopwords
# Tally the words
tidy_tweets_tokens <- 
  tidy_tweets %>%
  select(text, tweet_id, airline) %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  group_by(word) %>%
  tally() %>%
  ungroup
```

```{r}
# Stemmed version of words
tidy_tweets_stem <- 
  tidy_tweets %>%
  select(text, tweet_id, airline, tweet_created) %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  mutate(word = SnowballC::wordStem(word))
```

```{r}
# Get the number of words in each tweet 
text_stats <- 
  tidy_tweets_stem %>%
  group_by(tweet_id) %>%
  tally()

# Tweet Stats
summary(text_stats$n)
```

```{r}
# Lemmatized version of words 
tidy_tweets_lemma <-
  tidy_tweets %>%
  select(text, tweet_id, airline, tweet_created) %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  mutate(word = lemmatize_words(word)) 

# All versions of words 
all_tokens <-
  tidy_tweets_tokens %>%
  left_join(tidy_tweets_stem, by = "word")
```

```{r}
nrc <- get_sentiments("nrc")

# Gets nrc sentiments for each tweet
nrc_sentiment <-
  tidy_tweets %>%
  select(text, tweet_id) %>%
  unnest_tokens(word, text) %>%
  inner_join(get_sentiments("nrc")) %>%
  left_join(tidy_tweets, by = "tweet_id")
```

```{r}
# Sentiment counts by airline 
by_airline <- 
  nrc_sentiment %>%
  group_by(sentiment, airline) %>%
  tally()

ggplot(by_airline, aes(sentiment, n)) +
  geom_col() +
  facet_wrap(~airline) +
  coord_flip() +
  labs(title = "Total Sentiment by Airline") +
  xlab("Sentiment") +
  ylab("Total Sentiment Count") +
  theme_minimal()
```

```{r}
# Ratio of sentiments per airline
by_airline_2 <-
  by_airline %>%
  group_by(airline) %>%
  summarise(my_sum = sum(n)) %>%
  left_join(by_airline) %>%
  mutate(ratio = round((n/my_sum)*100))

by_airline_2 %>%
  ggplot(aes(sentiment, ratio, fill = ratio)) +
  geom_col(color = "white") +
  scale_fill_continuous_sequential(palette = "Purp") +
  geom_label(aes(label = ratio)) +
  labs(title = "Neg/Pos Ratio by Airline", fill = "Ratio percentage") +
  xlab("Sentiment") +
  ylab("Ratio") +
  facet_wrap(~airline) +
  coord_flip() +
  theme_minimal() +
  theme(legend.position = "bottom") 
```

##Analysis and Conclusions
For the sentiment analysis I used the NRC lexicon. This contains eight basic emotions: anger, fear, anticipation, trust, surprise, sadness, joy, and disgust. I felt like this lexicon was better suited for this dataset since the goal is to improve customer service. This will give much better insight into how the customers are feeling instead of just "positive" and "negative". In order to see the overall sentiment for each airline I did a ratio percentage for each airline for each emotion. Overall, the sentiment was similar for each airline. Virgin America had a slight lead with a 23% ratio for positive sentiments and United had a slight lead with 17% for negative sentiments. It is difficult to tell the exact reasons for these feelings, so it is hard to come up with solutions on exactly how to improve customer service. However, based off of the similar ratio for each airline, it appears to be an industry issue rather than just one specific airline. 
